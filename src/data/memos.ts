export interface Memo {
  number: string;
  slug: string;
  title: string;
  date: string;
  excerpt: string;
  tag: string;
  content: string[];
  relatedSlugs: string[];
}

export const memosData: Memo[] = [
  {
    number: "OCC-2026-003",
    slug: "escalation-problem",
    title: "The Escalation Problem: When AI Can't Say 'I Don't Know'",
    date: "Jan 2026",
    tag: "Trust Engineering",
    excerpt: "Decision systems require clear escalation paths. Current LLM deployments collapse this into a single probability distribution, creating brittleness at the edges of competence.",
    relatedSlugs: ["decision-budgets", "contestability"],
    content: [
      "Every functional organization has escalation paths. A junior analyst recognizes when a decision exceeds their authority or expertise and routes it upward. A customer service rep knows when to transfer to a specialist. These paths are explicit, institutional, and load-bearing.",
      "Current LLM deployments largely eliminate this architecture. The model either produces an answer or it doesn't. There's no structured mechanism for the system to recognize the boundaries of its competence and route to human judgment.",
      "This creates brittleness at the edges. The model's confidence distribution is continuous—it doesn't have a natural threshold for 'this requires escalation.' And even when confidence is low, the system typically still produces output rather than explicitly requesting human intervention.",
      "## The Confidence Problem",
      "Model confidence scores are poorly calibrated for escalation decisions. A model might be 60% confident in an answer that's completely wrong, or 40% confident in an answer that's correct. More problematically, the distribution of confidence scores doesn't map cleanly to the distribution of stakes.",
      "High-stakes decisions with narrow margins for error look the same to the model as low-stakes decisions where being approximately right is sufficient. The escalation logic must come from outside the model—from the system architecture that wraps it.",
      "## Designing Escalation Architecture",
      "Effective escalation requires three components:",
      "**Boundary recognition**: The system must be able to identify when a query or decision falls outside its reliable operating range. This can be done through explicit scope definition, anomaly detection on inputs, or calibrated uncertainty estimation.",
      "**Routing logic**: Once escalation is triggered, the system needs to know where to route. This means maintaining a map of human expertise and availability, with clear protocols for different escalation types.",
      "**Handoff protocols**: The transition from AI to human decision-making must preserve context. The human needs to understand what the AI was attempting, why it escalated, and what information is relevant to the decision.",
      "## Implementation Patterns",
      "We've observed several patterns that work in practice:",
      "**Explicit scope constraints**: Define the model's operating domain narrowly and route anything outside that domain to human review. This is conservative but reliable.",
      "**Calibrated thresholds**: Use held-out validation data to calibrate confidence thresholds for escalation. The threshold should be set based on acceptable error rates, not arbitrary confidence levels.",
      "**Structured uncertainty**: Instead of a single confidence score, decompose uncertainty into components—epistemic vs. aleatoric, input vs. model—and use different escalation logic for each type.",
      "## What This Means for Organizations",
      "If your AI deployment doesn't have explicit escalation architecture, you're implicitly trusting the model to know its limits. That's a decision you should make consciously, with eyes open to the failure modes it creates.",
      "Building proper escalation paths isn't just about preventing errors. It's about maintaining the institutional knowledge of when human judgment matters—knowledge that gets eroded when AI handles everything without distinction.",
    ],
  },
  {
    number: "OCC-2026-002",
    slug: "q4-review",
    title: "Compute CPI Q4 Review: Judgment Costs Outpace Moore's Law",
    date: "Dec 2025",
    tag: "Index Report",
    excerpt: "Despite hardware improvements, the cost of reliable AI judgment increased 12% YoY. The premium for trustworthy outputs is widening, not narrowing.",
    relatedSlugs: ["cognition-premium", "announcing-compute-cpi"],
    content: [
      "The headline Compute CPI closed Q4 2025 at 124.2, up 2.6% from Q3 and 12.1% year-over-year. This marks the fifth consecutive quarter of increases, defying expectations that compute costs would fall as hardware improved.",
      "The key finding: the Judgment CPI—our subindex tracking the cost of reliable, high-stakes AI outputs—increased 18.4% YoY, significantly outpacing the headline index. Organizations are paying more, not less, for AI they can trust.",
      "## Headline Numbers",
      "- **Occupant Index**: 124.2 (↑12.1% YoY)",
      "- **Judgment CPI**: 131.8 (↑18.4% YoY)",
      "- **Long Context CPI**: 118.4 (↑8.2% YoY)",
      "- **Budget CPI**: 106.2 (↑3.1% YoY)",
      "The divergence between Judgment CPI and Budget CPI widened to 25.6 points, up from 19.2 points at the start of 2025. This spread—what we call the Cognition Premium—represents the growing stratification in AI compute markets.",
      "## What's Driving the Increase?",
      "Three factors explain most of the movement:",
      "**1. Frontier model pricing power**: The major labs have maintained or increased pricing on their most capable models even as they've released them. GPT-4-class models cost roughly the same per token as they did at launch, while capabilities have improved. This is quality-adjusted price stability, not deflation.",
      "**2. Reliability requirements**: Enterprise deployments increasingly require consistency guarantees, retry logic, and human-in-the-loop verification. These operational costs add 20-40% to the raw API cost for judgment-critical applications.",
      "**3. Context economics**: Tasks that require long context windows face steeper pricing curves. The cost per marginal token increases non-linearly as context grows, and many judgment tasks require substantial context.",
      "## Budget Segment Deflation",
      "The one bright spot: commodity inference costs continue to fall. The Budget CPI increased only 3.1% YoY, and would have declined in absolute terms without quality adjustment. Open-source models and inference optimization have driven real cost reductions for simple tasks.",
      "This creates a bifurcated market. Organizations can do more low-stakes AI work for less money, while high-stakes work gets more expensive. The question for planning purposes: which category does your critical workload fall into?",
      "## Implications for 2026 Budgets",
      "Based on current trends, we project the headline index will reach 138-145 by end of 2026. The Judgment CPI could exceed 155 if current spreads persist.",
      "Organizations budgeting for AI should expect:",
      "- 15-20% annual cost increases for judgment-critical applications",
      "- Continued pressure to shift workloads toward cheaper model tiers",
      "- Growing operational costs for reliability and governance",
      "The assumption that 'AI will get cheaper' is true for some workloads and false for others. Budget accordingly.",
    ],
  },
  {
    number: "OCC-2026-001",
    slug: "decision-budgets",
    title: "Decision Budgets: A Framework for AI-Augmented Governance",
    date: "Nov 2025",
    tag: "Framework",
    excerpt: "Introducing the decision budget framework. How organizations should think about allocating judgment authority between humans and AI systems.",
    relatedSlugs: ["escalation-problem", "contestability"],
    content: [
      "Every organization has a finite capacity for human judgment. Executives can only review so many decisions. Experts can only evaluate so many cases. Committees can only meet so many times.",
      "This capacity is your decision budget. AI doesn't eliminate the need for human judgment—it changes how you allocate it.",
      "## The Core Insight",
      "Think of human judgment as a scarce resource to be allocated, not a default to be replaced. The question isn't 'can AI make this decision?' but 'where does human judgment add the most value?'",
      "A decision budget framework helps answer this by making explicit:",
      "- What decisions require human judgment",
      "- How much human judgment capacity exists",
      "- How that capacity should be allocated across decision types",
      "- What triggers reallocation as circumstances change",
      "## Framework Components",
      "**Decision Inventory**: Catalog all decisions your organization makes, classified by frequency, stakes, reversibility, and current decision-maker. This is the denominator of your budget.",
      "**Judgment Capacity**: Quantify how much human judgment capacity exists at each level of the organization. How many decisions can your senior leadership realistically evaluate per week? Your middle managers? Your frontline experts?",
      "**Allocation Rules**: Define which decisions require human judgment at each level, which can be delegated to AI with human oversight, and which can be fully automated. These rules should be explicit and documented.",
      "**Reallocation Triggers**: Specify what events should prompt budget reallocation. New risk categories, model failures, regulatory changes, or organizational growth all might require shifting human judgment to different decision types.",
      "## Practical Application",
      "We've implemented decision budgets with several organizations. Common patterns include:",
      "**Threshold-based allocation**: Decisions above certain stakes thresholds always get human review. This is simple but creates gaming incentives and doesn't account for decision complexity.",
      "**Sample-based allocation**: A random sample of AI decisions gets human review. This provides calibration data and maintains human expertise but doesn't catch individual high-risk decisions.",
      "**Anomaly-based allocation**: Decisions that look unusual—statistical outliers, novel situations, low confidence—get human review. This focuses human attention where it's most needed but requires good anomaly detection.",
      "Most organizations need a hybrid approach, with different rules for different decision categories.",
      "## The Governance Trap",
      "The most common failure mode: organizations implement AI without adjusting their decision budgets. Human judgment capacity stays allocated to decisions that are now automated, while new AI-generated decisions receive no human oversight.",
      "This is how you get simultaneously overworked managers and ungoverned AI systems. The budget wasn't reallocated—it was just stretched.",
      "## Getting Started",
      "If you're deploying AI into decision-making, start with the decision inventory. You can't allocate a budget you haven't measured.",
    ],
  },
  {
    number: "OCC-2025-011",
    slug: "cognition-premium",
    title: "The Cognition Premium: Why Frontier Model Costs Won't Fall Like You Expect",
    date: "Oct 2025",
    tag: "Index Report",
    excerpt: "Analysis of the widening spread between frontier and budget models. Capability stratification is creating a two-tier market with different economics.",
    relatedSlugs: ["q4-review", "announcing-compute-cpi"],
    content: [
      "The Cognition Premium—the cost spread between frontier reasoning models and budget alternatives—has widened to 25.7 index points, up from 18.3 points six months ago. This trend contradicts the common assumption that AI costs uniformly decline over time.",
      "## What We're Measuring",
      "The Cognition Premium tracks the additional cost of using frontier reasoning models (GPT-4, Claude Opus, Gemini Ultra class) compared to budget alternatives (GPT-3.5, Claude Haiku, open-source models) for equivalent tasks.",
      "For tasks that budget models can handle, this premium represents pure capability cost. For tasks that require frontier capabilities, it represents the cost of admission to reliable performance.",
      "## Why the Premium Is Widening",
      "**Capability stratification**: The gap between frontier and budget models is growing, not shrinking. Each generation of frontier models pulls further ahead on complex reasoning while budget models optimize for different trade-offs.",
      "**Pricing power**: Labs have discovered that customers who need frontier capabilities will pay for them. Enterprise contracts for top-tier models have held firm even as the broader market commoditizes.",
      "**Quality-adjusted costs**: When we adjust for capability improvements, frontier model costs look flat or increasing. You're paying the same for more capability, which means the capability itself is holding value.",
      "## Market Implications",
      "This creates a bifurcated market with different dynamics:",
      "**Budget tier**: Commoditizing rapidly. Open-source competition, inference optimization, and hardware improvements drive costs down. Good for high-volume, low-stakes workloads.",
      "**Frontier tier**: Pricing power persists. The labs that produce genuinely superior models can charge for that superiority. Organizations doing high-stakes work pay the premium.",
      "## Planning Implications",
      "If your AI roadmap assumes uniform cost deflation, revise it. The question is whether your workload requires frontier capabilities or can be served by budget alternatives. The answer determines which cost curve you're on.",
    ],
  },
  {
    number: "OCC-2025-010",
    slug: "contestability",
    title: "Contestability Architecture: Designing for Human Override",
    date: "Sep 2025",
    tag: "Trust Engineering",
    excerpt: "Technical patterns for building AI systems that remain contestable. How to preserve meaningful human agency in automated decision pipelines.",
    relatedSlugs: ["escalation-problem", "decision-budgets"],
    content: [
      "A decision system is contestable if affected parties can challenge its outputs through meaningful processes. This sounds obvious, but many AI deployments make contestability technically or practically impossible.",
      "## Why Contestability Matters",
      "Contestability isn't just an ethical nicety—it's load-bearing infrastructure for trust. Systems that can be challenged can also be corrected. Systems that can't be challenged accumulate errors that compound.",
      "For AI systems specifically, contestability serves multiple functions:",
      "- **Error correction**: Humans catch mistakes the system can't detect",
      "- **Legitimacy**: People accept decisions they can challenge",
      "- **Feedback loops**: Contested decisions provide training signal",
      "- **Accountability**: Someone must respond when decisions are wrong",
      "## The Contestability Stack",
      "Meaningful contestability requires multiple layers:",
      "**Visibility**: Affected parties must be able to see what decision was made and roughly why. This doesn't require full explainability, but it requires enough transparency that a challenge can be formulated.",
      "**Channels**: There must be clear pathways for raising challenges. Who do you contact? What information do they need? What's the expected timeline?",
      "**Authority**: Someone with authority to override the AI must receive and evaluate challenges. If the human in the loop can only rubber-stamp, contestability is theater.",
      "**Consequences**: Successful challenges must actually change outcomes. If the appeals process always confirms the original decision, people stop appealing.",
      "## Technical Patterns",
      "We've documented several patterns that support contestability:",
      "**Decision logging**: Every AI decision should be logged with enough context to reconstruct the decision basis later. This enables meaningful post-hoc review.",
      "**Override interfaces**: Build explicit override mechanisms into the system architecture. The ability to override should be a first-class feature, not an afterthought.",
      "**Confidence routing**: Route low-confidence decisions to human review by default. This concentrates human attention where contestability is most likely to be valuable.",
      "**Audit trails**: Maintain clear records of who reviewed what, when, and what they decided. This creates accountability for the humans in the loop, not just the AI.",
      "## What This Means in Practice",
      "If you can't answer the question 'how would someone challenge this AI decision?' with a concrete, operational answer, your system isn't contestable. Design the contestability architecture before you deploy, not after complaints arrive.",
    ],
  },
  {
    number: "OCC-2025-009",
    slug: "announcing-compute-cpi",
    title: "Announcing Compute CPI",
    date: "Aug 2025",
    tag: "Announcement",
    excerpt: "Introducing the Occupant Index: a public benchmark for the cost of AI work. Why we built it, how it works, and what it reveals.",
    relatedSlugs: ["q4-review", "cognition-premium"],
    content: [
      "Today we're publicly launching Compute CPI (the Occupant Index), a benchmark that tracks the cost of AI work using a market-basket methodology modeled on the Consumer Price Index.",
      "## Why We Built This",
      "Organizations making AI investment decisions face a fundamental question: how much will this cost next year? The common assumption is 'less,' but the reality is more nuanced.",
      "Existing benchmarks measure model capabilities—how well can GPT-4 solve math problems or write code. These are useful but don't answer the economic question. Compute CPI tracks what organizations actually pay to accomplish cognitive tasks.",
      "## How It Works",
      "We define a market basket of 47 standardized task categories across five segments: Frontier Reasoning, General Purpose, Budget Inference, Embedding & Search, and Multimodal.",
      "Each month, we price this basket using published API rates and enterprise contract data. The resulting index shows how the cost of doing the same work changes over time.",
      "We apply quality adjustment when models improve—if a model costs the same but performs 20% better, we treat that as a price decrease. This separates genuine cost changes from capability improvements.",
      "## What It Shows",
      "At launch, the headline index is set to 100 (base = January 2025). We've backdated calculations to show the trajectory over the past year.",
      "Key findings so far:",
      "- Headline costs are increasing, not decreasing, when quality-adjusted",
      "- The spread between frontier and budget models is widening",
      "- Judgment-critical tasks show steeper inflation than commodity inference",
      "- Long-context operations face non-linear cost scaling",
      "## Access",
      "The headline index is public. We publish monthly updates with MoM and YoY changes, plus quarterly reports with detailed analysis.",
      "Enterprise API access is available for organizations that want to integrate the index into their planning processes. Contact us for details.",
      "This is a public instrument. We built it because we think the market needs better information about AI economics. Use it.",
    ],
  },
];

export const getMemoBySlug = (slug: string): Memo | undefined => {
  return memosData.find((memo) => memo.slug === slug);
};

export const getRelatedMemos = (memo: Memo): Memo[] => {
  return memo.relatedSlugs
    .map((slug) => memosData.find((m) => m.slug === slug))
    .filter((m): m is Memo => m !== undefined);
};
